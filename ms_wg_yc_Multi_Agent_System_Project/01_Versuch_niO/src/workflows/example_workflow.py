"""LangGraph workflow that routes chat messages through an LLM component."""

from __future__ import annotations

import asyncio
import logging
from datetime import datetime
from typing import Any, Iterable, Sequence

from langchain_core.messages import AIMessage, AnyMessage, BaseMessage, HumanMessage
from langchain_core.runnables import Runnable
from langchain_core.tools import StructuredTool
from langgraph.graph import END, StateGraph
from langgraph.prebuilt import ToolNode

from llm.openrouter import build_openrouter_chat_model
from utils.tools import schedule_an_appointment

from .state import WorkflowState

logger = logging.getLogger(__name__)


class _EchoFallback:
    """Asynchronous echo-style responder used when the real LLM is unavailable."""

    is_fallback = True

    async def ainvoke(self, messages: Sequence[BaseMessage]) -> AIMessage:
        last_user_message = next(
            (msg.content for msg in reversed(messages) if isinstance(msg, HumanMessage)),
            "",
        )
        if last_user_message:
            text = "Echo fallback (set OPENROUTER_API_KEY for live responses): " f"{last_user_message}"
        else:
            text = "Echo fallback waiting for your first message."
        return AIMessage(content=text)


async def _invoke_model(llm: Runnable, messages: Iterable[AnyMessage]) -> AnyMessage:
    """Call ``ainvoke`` if available, otherwise run the synchronous ``invoke``."""
    if hasattr(llm, "ainvoke"):
        return await llm.ainvoke(messages)

    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(None, llm.invoke, messages)


def _coerce_to_ai_message(message: Any) -> AIMessage:
    """Ensure downstream code receives a proper AIMessage instance."""
    if isinstance(message, AIMessage):
        return message

    if hasattr(message, "content"):
        content = message.content  # type: ignore[attr-defined]
    else:
        content = str(message)

    return AIMessage(content=str(content))


def _try_build_default_llm() -> Runnable:
    """Return the default OpenRouter-backed LLM or fall back to echo."""
    try:
        return build_openrouter_chat_model(temperature=0.3)
    except Exception as exc:  # noqa: BLE001 - surface configuration issues gracefully
        logger.warning(
            "OpenRouter LLM unavailable; falling back to echo responder.",
            exc_info=exc,
        )
        return _EchoFallback()


def build_chat_workflow(llm: Runnable | None = None) -> Any:
    """Compile and return the chat workflow using the provided LLM component."""

    llm_component: Runnable = llm or _try_build_default_llm()

    schedule_an_appointment_tool = StructuredTool.from_function(
        schedule_an_appointment,
        name="schedule_an_appointment",
        description="Schedule an appointment in the user's calendar. The date should be provided in UTC format (e.g., '2025-09-24 15:00:00').",
    )

    tools = [schedule_an_appointment_tool]
    if tools and hasattr(llm_component, "bind_tools"):
        llm_component = llm_component.bind_tools(tools)
        logger.info(f"Configured {len(tools)} tool(s) for LLM")
    else:
        if not tools:
            logger.info("No tools configured; LLM will operate without tool calls.")
        else:
            logger.warning("LLM does not support tool binding; operating without tools.")

    tool_node = ToolNode(tools)

    async def generate_reply(state: WorkflowState) -> WorkflowState:
        messages = state.get("messages", [])
        ai_message = _coerce_to_ai_message(await _invoke_model(llm_component, messages))

        response_text = ai_message.content
        update: WorkflowState = {
            "messages": [ai_message],
            "response": response_text if isinstance(response_text, str) else str(response_text),
        }

        if getattr(llm_component, "is_fallback", False):
            update["notice"] = "OPENROUTER_API_KEY not detected. Responses are generated by the " "local echo fallback."

        return update

    # clarification_agent = ClarificationAgent()
    graph = StateGraph(WorkflowState)
    graph.add_node("model", generate_reply)
    graph.add_node("tools", tool_node)
    # graph.add_node("clarify", clarification_agent.run)  # Placeholder for future use
    graph.set_entry_point("model")
    graph.add_conditional_edges(
        "model",
        should_continue,
        {"tools": "tools", "end": END},
    )
    graph.add_edge("tools", "model")
    graph.add_edge("model", END)

    return graph.compile()


def should_continue(state: WorkflowState) -> str:
    last_message = state["messages"][-1]
    if last_message.tool_calls:
        return "tools"
    return "end"
